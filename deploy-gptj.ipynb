{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3038b765",
   "metadata": {},
   "source": [
    "## AIGC from Papers to Practice Series (1)\n",
    "\n",
    "# A Look at ChatGPT  &\u000b",
    "LLM Training Optimization with Amazon SageMaker\n",
    "\n",
    "### DEMO One: Build apps on open-source GPT-J Model with \u000b",
    "Amazon SageMaker\u000b",
    "\n",
    "\n",
    "Prepared by: Haowen Huang\n",
    "\n",
    "Feb 16, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1eb42",
   "metadata": {},
   "source": [
    "The goal of this demo is to help developers with little AI knowledge get started trying out ChatGPT-like technology, based on the open-source GPT-J model. At present it only goes as far as building the applications to get responses from input questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c44d2be",
   "metadata": {},
   "source": [
    "## GPT-J Model Overview\n",
    "\n",
    "GPT-J is a generative pretrained (GPT) language model and, in terms of its architecture, it’s comparable to popular, private, large language models like Open AI’s GPT-3. It consists of approximately 6 billion parameters and 28 layers, which consist of a feedforward block and a self-attention block. \n",
    "\n",
    "Serving GPT-J for inference has much lower memory requirements—in FP16, model weights occupy less than 13 GB, which means that inference can easily be conducted on a single 16 GB GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80dc5c",
   "metadata": {},
   "source": [
    "### Step 1: Start the SageMaker Instance\n",
    "\n",
    "1/ In AWS console, go to SageMaker and then click Notebook on the left.\n",
    "\n",
    "2/ Ensure you are in US-East-1 because that is where the S3 bucket is.\n",
    "\n",
    "3/ You'll need to create a new notebook instance and start it.\n",
    "\n",
    "4/ I used a ml.m5.4xlarge instance size. but remember I'm only using it for several minutes or hours and then you can stop the 2 instances (notebook and endpoint). \n",
    "\n",
    "5/ Click Open Jupyter (or Open JupyterLab)to launch the web interface.\n",
    "\n",
    "6/ Click New, and choose the following notebook type: conda_amazonei_pytorch_latest_p37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263457f2",
   "metadata": {},
   "source": [
    "### Step 2: Add Codes to Your Jupyter Notebook\n",
    "\n",
    "1/ Please update SageMaker Python SDK\n",
    "\n",
    "2/ Import HuggingFaceModel\n",
    "\n",
    "3/ Define the IAM role with permissions to create endpoint\n",
    "\n",
    "4/ Define the public S3 URI to GPT-J artifact\n",
    "\n",
    "5/ Create the Hugging Face model class\n",
    "\n",
    "6/ Deploy model to SageMaker Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2987f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker\n",
    "\n",
    "# IAM role with permissions to create endpoint\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# public S3 URI to gpt-j artifact\n",
    "model_uri=\"s3://huggingface-sagemaker-models/transformers/4.12.3/pytorch/1.9.1/gpt-j/model.tar.gz\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=model_uri,\n",
    "    transformers_version='4.12.3',\n",
    "    pytorch_version='1.9.1',\n",
    "    py_version='py38',\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1, # number of instances\n",
    "    instance_type='ml.g4dn.xlarge' #'ml.p3.2xlarge' # ec2 instance type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecec9eb",
   "metadata": {},
   "source": [
    "Then, run all code above. (Use the Run button.) You will see this output: ----------------! Please note that as GPT loads, you will see dashes (\"-\") added to the output.\n",
    "\n",
    "Common error: Ensure you are in US-East-1 since that is where the S3 bucket is.\n",
    "\n",
    "Next add this and run it.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9be7dd",
   "metadata": {},
   "source": [
    "### Step 3: Run the Prediction\n",
    "\n",
    "Here is the step three: run your own prediction with GPT-J model!\n",
    "\n",
    "My own experience is that it takes about 10 minutes to complete the deployment of this demo, so we can start predicting. Let's give you 20 minutes to experience the whole process of deploying the GPT-J model on Amazon SageMaker and completing the Q&A reasoning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01846282",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict({\n",
    "    \"inputs\": \"The tallest building in Hong Kong is\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e5fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict({\n",
    "    \"inputs\": \"The most expensive property in Hong Kong is\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e58bbad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
